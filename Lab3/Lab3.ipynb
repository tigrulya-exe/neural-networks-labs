{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"../images/sberbank.png\">\n",
    "# <center> Лабораторная 3 </center>\n",
    "## <center> Предсказание пола клиента по транзакциям</center>\n",
    "\n",
    "## Описание задачи \n",
    "### В роли метрики выступает [ROC AUC](https://dyakonov.org/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/), который и нужно будет оптимизировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:34:52.054851Z",
     "start_time": "2019-03-05T17:34:51.515762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-e80c5b462489>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from warnings import filterwarnings\n",
    "\n",
    "%matplotlib inline\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:34:58.314241Z",
     "start_time": "2019-03-05T17:34:52.056205Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Считываем данные\n",
    "tr_mcc_codes = pd.read_csv('data/tr_mcc_codes.csv', sep=';', index_col='mcc_code')\n",
    "tr_types = pd.read_csv('data/tr_types.csv', sep=';', index_col='tr_type')\n",
    "\n",
    "transactions = pd.read_csv('data/transactions.csv', index_col='customer_id')\n",
    "gender_train = pd.read_csv('data/gender_train.csv', index_col='customer_id')\n",
    "gender_test = pd.read_csv('data/gender_test.csv', index_col='customer_id')\n",
    "transactions_train = transactions.join(gender_train, how='inner')\n",
    "transactions_test = transactions.join(gender_test, how='inner')\n",
    "\n",
    "del transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:34:58.354996Z",
     "start_time": "2019-03-05T17:34:58.315591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Функции, которыми можно пользоваться для построения классификатора, \n",
    "# оценки его результатов и построение прогноза для тестовой части пользователей\n",
    "\n",
    "# Cross-validation score (среднее значение метрики ROC AUC на тренировочных данных)\n",
    "def cv_score(params, train, y_true):\n",
    "    cv_res=xgb.cv(params, xgb.DMatrix(train, y_true),\n",
    "                  early_stopping_rounds=10, maximize=True, \n",
    "                  num_boost_round=10000, nfold=5, stratified=True)\n",
    "    index_argmax = cv_res['test-auc-mean'].argmax()\n",
    "    print('Cross-validation, ROC AUC: {:.3f}+-{:.3f}, Trees: {}'.format(cv_res.loc[index_argmax]['test-auc-mean'],\n",
    "                                                                        cv_res.loc[index_argmax]['test-auc-std'],\n",
    "                                                                        index_argmax))\n",
    "\n",
    "# Построение модели + возврат результатов классификации тестовых пользователей\n",
    "def fit_predict(params, num_trees, train, test, target):\n",
    "    params['learning_rate'] = params['eta']\n",
    "    clf = xgb.train(params, xgb.DMatrix(train.values, target, feature_names=list(train.columns)), \n",
    "                    num_boost_round=num_trees, maximize=True)\n",
    "    y_pred = clf.predict(xgb.DMatrix(test.values, feature_names=list(train.columns)))\n",
    "    submission = pd.DataFrame(index=test.index, data=y_pred, columns=['probability'])\n",
    "    return clf, submission\n",
    "\n",
    "# Отрисовка важности переменных. Важность переменной - количество разбиений выборки, \n",
    "# в которых участвует данная переменная. Чем больше - тем она, вероятно, лучше \n",
    "def draw_feature_importances(clf, top_k=10):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    importances = dict(sorted(clf.get_score().items(), key=lambda x: x[1])[-top_k:])\n",
    "    y_pos = np.arange(len(importances))\n",
    "    \n",
    "    plt.barh(y_pos, list(importances.values()), align='center', color='green')\n",
    "    plt.yticks(y_pos, importances.keys(), fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.xlabel('Feature importance', fontsize=15)\n",
    "    plt.title('Features importances, Sberbank Gender Prediction', fontsize=18)\n",
    "    plt.ylim(-0.5, len(importances) - 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Так как код для оценки модели на тренировочных данных и её применения на тестовых данных уже дан, то мы будем работать над тем, чтобы создать переменные для улучшения результатов моделирования. \n",
    "\n",
    "### (!) В рамках данного задания Вы можете делать всё, что угодно - использовать другие алгоритмы и/или их комбинации, подбирать гиперпараметры своих моделей, отбирать переменые, etc. Мы создали шаблон для простоты и для Вашего понимания верхнеуровневого процесса разработки модели, опустив при этом большое число деталей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic features\n",
    "Начнём с того, что сформируем базовые переменные по каждому пользователю. На этом этапе будем использовать стандартные агрегации, посчитанные на расходах и приходах клиента:\n",
    "- минимум\n",
    "- максимум\n",
    "- среднее\n",
    "- медиана\n",
    "- среднеквадратичное отклонение\n",
    "- количество\n",
    "\n",
    "Также параметры модели выберем стандартные, запишем их в словарь params, и будем использовать для дальнейшего построения модели (не забывайте, что с этим Вы можете тоже экспериментировать)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:34:58.362104Z",
     "start_time": "2019-03-05T17:34:58.356308Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    \n",
    "    'gamma': 0,\n",
    "    'lambda': 0,\n",
    "    'alpha': 0,\n",
    "    'min_child_weight': 0,\n",
    "    \n",
    "    'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic' ,\n",
    "    'booster': 'gbtree',\n",
    "    'njobs': -1,\n",
    "    'tree_method': 'approx'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:35:41.272300Z",
     "start_time": "2019-03-05T17:34:58.363622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Progress::   0%|          | 0/8400 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4374d28b9d34c2ab306a11bbdd20973"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Progress::   0%|          | 0/3600 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1158d2edd6541a08fe13024f8e30431"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm_notebook.pandas(desc=\"Progress:\")\n",
    "\n",
    "def features_creation_basic(x): \n",
    "    features = []\n",
    "    features.append(pd.Series(x[x['amount']>0]['amount'].agg(['min', 'max', 'mean', 'median', 'std', 'count'])\\\n",
    "                                                        .add_prefix('positive_transactions_')))\n",
    "    features.append(pd.Series(x[x['amount']<0]['amount'].agg(['min', 'max', 'mean', 'median', 'std', 'count'])\\\n",
    "                                                        .add_prefix('negative_transactions_')))\n",
    " \n",
    "    return pd.concat(features)\n",
    "\n",
    "data_train = transactions_train.groupby(transactions_train.index).progress_apply(features_creation_basic)\n",
    "data_test = transactions_test.groupby(transactions_test.index).progress_apply(features_creation_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:35:45.011046Z",
     "start_time": "2019-03-05T17:35:41.273326Z"
    }
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "sklearn needs to be installed in order to use stratified cv",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mXGBoostError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-4c3def387e04>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata_train\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgender_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'inner'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'gender'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mcv_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-9-1f636552c46a>\u001B[0m in \u001B[0;36mcv_score\u001B[1;34m(params, train, y_true)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m# Cross-validation score (среднее значение метрики ROC AUC на тренировочных данных)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mcv_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_true\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     cv_res=xgb.cv(params, xgb.DMatrix(train, y_true),\n\u001B[0m\u001B[0;32m      7\u001B[0m                   \u001B[0mearly_stopping_rounds\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaximize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m                   num_boost_round=10000, nfold=5, stratified=True)\n",
      "\u001B[1;32mc:\\users\\sander\\venv\\lib\\site-packages\\xgboost\\training.py\u001B[0m in \u001B[0;36mcv\u001B[1;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001B[0m\n\u001B[0;32m    443\u001B[0m     \"\"\"\n\u001B[0;32m    444\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mstratified\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mTrue\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mSKLEARN_INSTALLED\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 445\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mXGBoostError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'sklearn needs to be installed in order to use stratified cv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    446\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    447\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mXGBoostError\u001B[0m: sklearn needs to be installed in order to use stratified cv"
     ]
    }
   ],
   "source": [
    "target = data_train.join(gender_train, how='inner')['gender']\n",
    "cv_score(params, data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:35:45.714620Z",
     "start_time": "2019-03-05T17:35:45.018570Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-de4eff64b516>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m### Число деревьев для XGBoost имеет смысл выставлять по результатам на кросс-валидации\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mclf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msubmission\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfit_predict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m70\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "### Число деревьев для XGBoost имеет смысл выставлять по результатам на кросс-валидации \n",
    "clf, submission = fit_predict(params, 70, data_train, data_test, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:35:45.897370Z",
     "start_time": "2019-03-05T17:35:45.719320Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-bc04db43f68a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdraw_feature_importances\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "draw_feature_importances(clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../data/basic_features_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Видим, что результат на кросс-валидации - 62.5% ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced features\n",
    "Добавим дополнительные переменные по каждому пользователю в модель. <br>\n",
    "Для этого будем анализировать дни недели, часы и состояние дня/ночи во время покупки - в каждом из случаев будем считать частоту транзакций в соответствующей категории относитеьно всех остальных категорий. <br>\n",
    "То есть если, например, клиент в 70% случае совершал ночные траты, то мы получим вектор [0.7, 0.3] для этого случая в качестве частот транзакций ночью/днём."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:35:57.677748Z",
     "start_time": "2019-03-05T17:35:45.921444Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in [transactions_train, transactions_test]:\n",
    "    df['day'] = df['tr_datetime'].str.split().apply(lambda x: int(x[0]) % 7)\n",
    "    df['hour'] = df['tr_datetime'].apply(lambda x: re.search(' \\d*', x).group(0)).astype(int)\n",
    "    df['night'] = ~df['hour'].between(6, 22).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:35:57.687397Z",
     "start_time": "2019-03-05T17:35:57.678823Z"
    }
   },
   "outputs": [],
   "source": [
    "def features_creation_advanced(x): \n",
    "    features = []\n",
    "    features.append(pd.Series(x['day'].value_counts(normalize=True).add_prefix('day_')))\n",
    "    features.append(pd.Series(x['hour'].value_counts(normalize=True).add_prefix('hour_')))\n",
    "    features.append(pd.Series(x['night'].value_counts(normalize=True).add_prefix('night_')))\n",
    "    features.append(pd.Series(x[x['amount']>0]['amount'].agg(['min', 'max', 'mean', 'median', 'std', 'count'])\\\n",
    "                                                        .add_prefix('positive_transactions_')))\n",
    "    features.append(pd.Series(x[x['amount']<0]['amount'].agg(['min', 'max', 'mean', 'median', 'std', 'count'])\\\n",
    "                                                        .add_prefix('negative_transactions_')))\n",
    " \n",
    "    return pd.concat(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:37:19.784321Z",
     "start_time": "2019-03-05T17:35:57.688448Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = transactions_train.groupby(transactions_train.index)\\\n",
    "                               .progress_apply(features_creation_advanced).unstack(-1)\n",
    "data_test = transactions_test.groupby(transactions_test.index)\\\n",
    "                             .progress_apply(features_creation_advanced).unstack(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:37:27.200528Z",
     "start_time": "2019-03-05T17:37:19.785535Z"
    }
   },
   "outputs": [],
   "source": [
    "target = data_train.join(gender_train, how='inner')['gender']\n",
    "cv_score(params, data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:37:28.307548Z",
     "start_time": "2019-03-05T17:37:27.205214Z"
    }
   },
   "outputs": [],
   "source": [
    "### Число деревьев для XGBoost имеет смысл выятавлять по результатам на кросс-валидации \n",
    "clf, submission = fit_predict(params, 70, data_train, data_test, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:37:28.426836Z",
     "start_time": "2019-03-05T17:37:28.308620Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_feature_importances(clf, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Добавление новых переменных улучшило наши результаты ROC AUC с 62.5% до 68.2%, на тестовой выборке результат будет аналогичным, так что мы явно не переобучились. При этом есть куда стремиться!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T17:37:28.448839Z",
     "start_time": "2019-03-05T17:37:28.438234Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('../data/submission_advanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (!) Цель задания:\n",
    "## Полученная модель должна иметь ROC AUC на Public-части тестовой выборки (на лидерборде) не менее 80%."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}